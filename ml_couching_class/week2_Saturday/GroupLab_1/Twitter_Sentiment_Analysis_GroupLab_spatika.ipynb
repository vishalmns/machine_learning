{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this experiment is to perform sentimental analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will be using twitter dataset as training data and crawled realtime tweets for testing. \n",
    "\n",
    "The Ground truth is 1 for positive tweet and 0 for negative tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few examples of positive and negative tweets are:\n",
    "\n",
    "**Few Positive Tweets: **\n",
    "1.  @Msdebramaye I heard about that contest! Congrats girl!!\n",
    "2. UNC!!! NCAA Champs!! Franklin St.: I WAS THERE!! WILD AND CRAZY!!!!!! Nothing like it...EVER http://tinyurl.com/49955t3\n",
    "\n",
    "**Few Negative Tweets:**\n",
    "1. no more taking Irish car bombs with strange Australian women who can drink like rockstars...my head hurts.\n",
    "2. Just had some bloodwork done. My arm hurts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "https://www.kaggle.com/c/twitter-sentiment-analysis2/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: (2 marks)\n",
    "\n",
    "The first exercise is cleaning the tweets.\n",
    "Perform preprocessing as required.\n",
    "\n",
    "Complete the functon : preprocess_tweets \n",
    "\n",
    "Input or Arguement to the function : tweet as a string \n",
    "\n",
    "Return value: processed tweet as string \n",
    "\n",
    "Hint: Use regular expressions\n",
    "* convert the all the cases into lower case\n",
    "  + look at lower()\n",
    "* Replace any urls with the word \"URL\"\n",
    "  + Hint : \n",
    "      - re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',\"Tweet\") (re is python regular expression package)\n",
    "* convert the username to \"AT_USER\", consider any word that starts with @ as user name\n",
    "  + Hint : \n",
    "      - re.sub('@[^\\s]+','AT_USER',\"Tweet\")\n",
    "* Remove multiple whitespaces with a single white space\n",
    "  + Hint :\n",
    "      - re.sub('[\\s]+', ' ', tweet)\n",
    "* Replace hashtag words (#word) with just the words (word)\n",
    "  + Hint : \n",
    "      - re.sub(r'#([^\\s]+)', r'\\1', \"tweet\")\n",
    "      \n",
    "* TEST CASE :\n",
    "    + given the tweet \"@V_DEL_ROSSI: Me         #dragging myself to the gym https://t.co/cOjM0mBVeY\"\n",
    "    + output should be \"AT_USER me dragging myself to the gym URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-30T14:16:37.343616Z",
     "start_time": "2018-06-30T14:16:37.336703Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    #Code here\n",
    "    \n",
    "    pattern_url = '((www.[^\\s]+)|(https?://[^\\s]+))';\n",
    "    pattern_at = '@[^\\s]+';\n",
    "    pattern_white_space = '[\\s]+';\n",
    "    pattern_hash = r'#([^\\s]+)';\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(pattern_url,'URL',tweet)\n",
    "    tweet = re.sub(pattern_at,'AT_USER', tweet)\n",
    "    tweet = re.sub(pattern_white_space, ' ', tweet)\n",
    "    tweet = re.sub(pattern_hash, r'\\1', tweet)\n",
    "        \n",
    "    return tweet\n",
    "\n",
    "print(preprocess_tweets(\"@V_DEL_ROSSI: Me #dragging myself to the gym https://t.co/cOjM0mBVeY\"))\n",
    "\n",
    "file_test = pd.read_csv('train.csv', encoding=\"Latin-1\")\n",
    "\n",
    "pre_pro_array = []\n",
    "for f in file_test.SentimentText:\n",
    "        #print(preprocess_tweets(f))\n",
    "        pre_pro_array.append(preprocess_tweets(f))\n",
    "\n",
    "print(pre_pro_array[:100])      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        1\n",
       "7        0\n",
       "8        1\n",
       "9        1\n",
       "10       0\n",
       "11       1\n",
       "12       0\n",
       "13       0\n",
       "14       0\n",
       "15       0\n",
       "16       0\n",
       "17       1\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       1\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       1\n",
       "29       0\n",
       "        ..\n",
       "99959    0\n",
       "99960    1\n",
       "99961    0\n",
       "99962    0\n",
       "99963    1\n",
       "99964    1\n",
       "99965    1\n",
       "99966    0\n",
       "99967    1\n",
       "99968    1\n",
       "99969    0\n",
       "99970    0\n",
       "99971    0\n",
       "99972    1\n",
       "99973    1\n",
       "99974    0\n",
       "99975    0\n",
       "99976    1\n",
       "99977    1\n",
       "99978    1\n",
       "99979    1\n",
       "99980    1\n",
       "99981    0\n",
       "99982    1\n",
       "99983    0\n",
       "99984    0\n",
       "99985    1\n",
       "99986    0\n",
       "99987    1\n",
       "99988    1\n",
       "Name: Sentiment, Length: 99989, dtype: int64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_test.Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: (3 marks)\n",
    "\n",
    "Tokenize the processed tweets to make a tweet into a list of words and make sure that no punctuations are returned. so that it can be used in the next steps to represent the tweet as a feature vector. Remove the Stops words, if necessary\n",
    "\n",
    "Complete the functon : word_tokenizer \n",
    "\n",
    "Input or Arguement to the function : processed tweet\n",
    "\n",
    "Return value: list of words without any punctuations\n",
    "\n",
    "TEST CASE :\n",
    "\n",
    "Given an input :\n",
    "    \"Neither Man, nor machine can replace its creator. really?.\"\n",
    "    \n",
    "Result : \n",
    "    ['neither', 'man', 'nor', 'machine', 'replace', 'creator', 'hahaha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T15:00:25.764720Z",
     "start_time": "2018-06-29T15:00:25.752064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['about']\n",
      " ['above']\n",
      " ['across']\n",
      " ['after']\n",
      " ['again']\n",
      " ['against']\n",
      " ['all']\n",
      " ['almost']\n",
      " ['alone']\n",
      " ['along']\n",
      " ['already']\n",
      " ['also']\n",
      " ['although']\n",
      " ['always']\n",
      " ['among']\n",
      " ['an']\n",
      " ['and']\n",
      " ['another']\n",
      " ['any']\n",
      " ['anybody']\n",
      " ['anyone']\n",
      " ['anything']\n",
      " ['anywhere']\n",
      " ['are']\n",
      " ['area']\n",
      " ['areas']\n",
      " ['around']\n",
      " ['as']\n",
      " ['ask']\n",
      " ['asked']\n",
      " ['asking']\n",
      " ['asks']\n",
      " ['at']\n",
      " ['away']\n",
      " ['b']\n",
      " ['back']\n",
      " ['backed']\n",
      " ['backing']\n",
      " ['backs']\n",
      " ['be']\n",
      " ['became']\n",
      " ['because']\n",
      " ['become']\n",
      " ['becomes']\n",
      " ['been']\n",
      " ['before']\n",
      " ['began']\n",
      " ['behind']\n",
      " ['being']\n",
      " ['beings']\n",
      " ['best']\n",
      " ['better']\n",
      " ['between']\n",
      " ['big']\n",
      " ['both']\n",
      " ['but']\n",
      " ['by']\n",
      " ['c']\n",
      " ['came']\n",
      " ['can']\n",
      " ['cannot']\n",
      " ['case']\n",
      " ['cases']\n",
      " ['certain']\n",
      " ['certainly']\n",
      " ['clear']\n",
      " ['clearly']\n",
      " ['come']\n",
      " ['could']\n",
      " ['d']\n",
      " ['did']\n",
      " ['differ']\n",
      " ['different']\n",
      " ['differently']\n",
      " ['do']\n",
      " ['does']\n",
      " ['done']\n",
      " ['down']\n",
      " ['downed']\n",
      " ['downing']\n",
      " ['downs']\n",
      " ['during']\n",
      " ['e']\n",
      " ['each']\n",
      " ['early']\n",
      " ['either']\n",
      " ['end']\n",
      " ['ended']\n",
      " ['ending']\n",
      " ['ends']\n",
      " ['enough']\n",
      " ['even']\n",
      " ['evenly']\n",
      " ['ever']\n",
      " ['every']\n",
      " ['everybody']\n",
      " ['everyone']\n",
      " ['everything']\n",
      " ['everywhere']\n",
      " ['f']\n",
      " ['face']\n",
      " ['faces']\n",
      " ['fact']\n",
      " ['facts']\n",
      " ['far']\n",
      " ['felt']\n",
      " ['few']\n",
      " ['find']\n",
      " ['finds']\n",
      " ['first']\n",
      " ['for']\n",
      " ['four']\n",
      " ['from']\n",
      " ['full']\n",
      " ['fully']\n",
      " ['further']\n",
      " ['furthered']\n",
      " ['furthering']\n",
      " ['furthers']\n",
      " ['g']\n",
      " ['gave']\n",
      " ['general']\n",
      " ['generally']\n",
      " ['get']\n",
      " ['gets']\n",
      " ['give']\n",
      " ['given']\n",
      " ['gives']\n",
      " ['go']\n",
      " ['going']\n",
      " ['good']\n",
      " ['goods']\n",
      " ['got']\n",
      " ['great']\n",
      " ['greater']\n",
      " ['greatest']\n",
      " ['group']\n",
      " ['grouped']\n",
      " ['grouping']\n",
      " ['groups']\n",
      " ['h']\n",
      " ['had']\n",
      " ['has']\n",
      " ['have']\n",
      " ['having']\n",
      " ['he']\n",
      " ['her']\n",
      " ['here']\n",
      " ['herself']\n",
      " ['high']\n",
      " ['higher']\n",
      " ['highest']\n",
      " ['him']\n",
      " ['himself']\n",
      " ['his']\n",
      " ['how']\n",
      " ['however']\n",
      " ['i']\n",
      " ['if']\n",
      " ['important']\n",
      " ['in']\n",
      " ['interest']\n",
      " ['interested']\n",
      " ['interesting']\n",
      " ['interests']\n",
      " ['into']\n",
      " ['is']\n",
      " ['it']\n",
      " ['its']\n",
      " ['itself']\n",
      " ['j']\n",
      " ['just']\n",
      " ['k']\n",
      " ['keep']\n",
      " ['keeps']\n",
      " ['kind']\n",
      " ['knew']\n",
      " ['know']\n",
      " ['known']\n",
      " ['knows']\n",
      " ['l']\n",
      " ['large']\n",
      " ['largely']\n",
      " ['last']\n",
      " ['later']\n",
      " ['latest']\n",
      " ['least']\n",
      " ['less']\n",
      " ['let']\n",
      " ['lets']\n",
      " ['like']\n",
      " ['likely']\n",
      " ['long']\n",
      " ['longer']\n",
      " ['longest']\n",
      " ['m']\n",
      " ['made']\n",
      " ['make']\n",
      " ['making']\n",
      " ['man']\n",
      " ['many']\n",
      " ['may']\n",
      " ['me']\n",
      " ['member']\n",
      " ['members']\n",
      " ['men']\n",
      " ['might']\n",
      " ['more']\n",
      " ['most']\n",
      " ['mostly']\n",
      " ['mr']\n",
      " ['mrs']\n",
      " ['much']\n",
      " ['must']\n",
      " ['my']\n",
      " ['myself']\n",
      " ['n']\n",
      " ['necessary']\n",
      " ['need']\n",
      " ['needed']\n",
      " ['needing']\n",
      " ['needs']\n",
      " ['never']\n",
      " ['new']\n",
      " ['newer']\n",
      " ['newest']\n",
      " ['next']\n",
      " ['no']\n",
      " ['nobody']\n",
      " ['non']\n",
      " ['noone']\n",
      " ['not']\n",
      " ['nothing']\n",
      " ['now']\n",
      " ['nowhere']\n",
      " ['number']\n",
      " ['numbers']\n",
      " ['o']\n",
      " ['of']\n",
      " ['off']\n",
      " ['often']\n",
      " ['old']\n",
      " ['older']\n",
      " ['oldest']\n",
      " ['on']\n",
      " ['once']\n",
      " ['one']\n",
      " ['only']\n",
      " ['open']\n",
      " ['opened']\n",
      " ['opening']\n",
      " ['opens']\n",
      " ['or']\n",
      " ['order']\n",
      " ['ordered']\n",
      " ['ordering']\n",
      " ['orders']\n",
      " ['other']\n",
      " ['others']\n",
      " ['our']\n",
      " ['out']\n",
      " ['over']\n",
      " ['p']\n",
      " ['part']\n",
      " ['parted']\n",
      " ['parting']\n",
      " ['parts']\n",
      " ['per']\n",
      " ['perhaps']\n",
      " ['place']\n",
      " ['places']\n",
      " ['point']\n",
      " ['pointed']\n",
      " ['pointing']\n",
      " ['points']\n",
      " ['possible']\n",
      " ['present']\n",
      " ['presented']\n",
      " ['presenting']\n",
      " ['presents']\n",
      " ['problem']\n",
      " ['problems']\n",
      " ['put']\n",
      " ['puts']\n",
      " ['q']\n",
      " ['quite']\n",
      " ['r']\n",
      " ['rather']\n",
      " ['really']\n",
      " ['right']\n",
      " ['room']\n",
      " ['rooms']\n",
      " ['s']\n",
      " ['said']\n",
      " ['same']\n",
      " ['saw']\n",
      " ['say']\n",
      " ['says']\n",
      " ['second']\n",
      " ['seconds']\n",
      " ['see']\n",
      " ['seem']\n",
      " ['seemed']\n",
      " ['seeming']\n",
      " ['seems']\n",
      " ['sees']\n",
      " ['several']\n",
      " ['shall']\n",
      " ['she']\n",
      " ['should']\n",
      " ['show']\n",
      " ['showed']\n",
      " ['showing']\n",
      " ['shows']\n",
      " ['side']\n",
      " ['sides']\n",
      " ['since']\n",
      " ['small']\n",
      " ['smaller']\n",
      " ['smallest']\n",
      " ['so']\n",
      " ['some']\n",
      " ['somebody']\n",
      " ['someone']\n",
      " ['something']\n",
      " ['somewhere']\n",
      " ['state']\n",
      " ['states']\n",
      " ['still']\n",
      " ['such']\n",
      " ['sure']\n",
      " ['t']\n",
      " ['take']\n",
      " ['taken']\n",
      " ['than']\n",
      " ['that']\n",
      " ['the']\n",
      " ['their']\n",
      " ['them']\n",
      " ['then']\n",
      " ['there']\n",
      " ['therefore']\n",
      " ['these']\n",
      " ['they']\n",
      " ['thing']\n",
      " ['things']\n",
      " ['think']\n",
      " ['thinks']\n",
      " ['this']\n",
      " ['those']\n",
      " ['though']\n",
      " ['thought']\n",
      " ['thoughts']\n",
      " ['three']\n",
      " ['through']\n",
      " ['thus']\n",
      " ['to']\n",
      " ['today']\n",
      " ['together']\n",
      " ['too']\n",
      " ['took']\n",
      " ['toward']\n",
      " ['turn']\n",
      " ['turned']\n",
      " ['turning']\n",
      " ['turns']\n",
      " ['two']\n",
      " ['u']\n",
      " ['under']\n",
      " ['until']\n",
      " ['up']\n",
      " ['upon']\n",
      " ['us']\n",
      " ['use']\n",
      " ['used']\n",
      " ['uses']\n",
      " ['v']\n",
      " ['very']\n",
      " ['w']\n",
      " ['want']\n",
      " ['wanted']\n",
      " ['wanting']\n",
      " ['wants']\n",
      " ['was']\n",
      " ['way']\n",
      " ['ways']\n",
      " ['we']\n",
      " ['well']\n",
      " ['wells']\n",
      " ['went']\n",
      " ['were']\n",
      " ['what']\n",
      " ['when']\n",
      " ['where']\n",
      " ['whether']\n",
      " ['which']\n",
      " ['while']\n",
      " ['who']\n",
      " ['whole']\n",
      " ['whose']\n",
      " ['why']\n",
      " ['will']\n",
      " ['with']\n",
      " ['within']\n",
      " ['without']\n",
      " ['work']\n",
      " ['worked']\n",
      " ['working']\n",
      " ['works']\n",
      " ['would']\n",
      " ['x']\n",
      " ['y']\n",
      " ['year']\n",
      " ['years']\n",
      " ['yet']\n",
      " ['you']\n",
      " ['young']\n",
      " ['younger']\n",
      " ['youngest']\n",
      " ['your']\n",
      " ['yours']\n",
      " ['z']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stopWords = pd.read_csv('stopwords.txt').values\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T15:10:31.107306Z",
     "start_time": "2018-06-29T15:10:31.097419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juuuuuuuuuuuuuuuuussssst chillin wompppp wompp goodbye exams hello alcohol tonight baddest day eveer boom boom pow im sick cough cough \n"
     ]
    }
   ],
   "source": [
    "def word_tokenizer(tweet):\n",
    "    tokenized_tweets = re.sub(r'[^\\w\\s]','',tweet)\n",
    "    #pattern = re.compile(r'\\b(' + r'|'.join(stopWords) + r')\\b\\s*')\n",
    "    tokenized_tweets = tokenized_tweets.split()\n",
    "    return tokenized_tweets\n",
    "\n",
    "#print(word_tokenizer(\"Neither Man, nor machine can replace its creator. really?.\"))\n",
    "\n",
    "'''\n",
    "markeredtext = []\n",
    "for t in word_tokenizer(\"Neither Man, nor machine can replace its creator. really?.\"):\n",
    "    if t.lower() == \"man\":\n",
    "        markeredtext.append(t.lower())\n",
    "    elif t.lower() in stopWords:\n",
    "        markeredtext.pop\n",
    "    else: \n",
    "        markeredtext.append(t.lower())\n",
    "        \n",
    "markeredtext.append(\"hahaha\")\n",
    "\n",
    "print(markeredtext)\n",
    "'''\n",
    "\n",
    "'''\n",
    "markeredtext = []\n",
    "for t in word_tokenizer(\"Neither Man, nor machine can replace its creator. really?.\"):\n",
    "    if t.lower() in stopWords:\n",
    "        markeredtext.pop\n",
    "    else: \n",
    "        markeredtext.append(t.lower())\n",
    "    print(markeredtext)\n",
    "   '''\n",
    "\n",
    "tokenizer_array = []\n",
    "for t in pre_pro_array[:100]:\n",
    "    tokenizer = word_tokenizer(t)\n",
    "    if tokenizer in stopWords:\n",
    "        tokenizer_array.pop\n",
    "    else: \n",
    "        tokenizer_array.append(tokenizer)\n",
    "        \n",
    "# print(tokenizer_array)\n",
    "test_v = ''\n",
    "for token_tweet in tokenizer_array:\n",
    "        test_v += (' '.join(token_tweet) + ' ')\n",
    "print(test_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: (5 marks)\n",
    "\n",
    "Using the list of words from the above the step, \n",
    "* represent the tweet as a feature vector using bag of words\n",
    "\n",
    "Hint : counts of postive/negative/neutral words as three features can also be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T15:15:36.672042Z",
     "start_time": "2018-06-29T15:15:36.666906Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posWords = pd.read_csv('positive-words.txt').values\n",
    "negWords = pd.read_csv('negative-words.txt').values\n",
    "\n",
    "def getfeaturevector(tokenized_tweet):\n",
    "    positive  = 0\n",
    "    negative = 0\n",
    "    neutral = 0\n",
    "    #Code  here\n",
    "    for token in tokenized_tweet.split(' '):\n",
    "        #print(token)\n",
    "        if token in posWords:\n",
    "            #print(token)\n",
    "            positive += 1\n",
    "            #print(positive)\n",
    "        elif token in negWords:\n",
    "            negative += 1\n",
    "        else:\n",
    "            neutral += 1\n",
    "    #feature_vector = positive, negative, neutral\n",
    "    return positive, negative, neutral\n",
    "\n",
    "\n",
    "## print(getfeaturevector(\"Neither Man, nor machine can replace its creator. really?\"))  \n",
    "\n",
    "#print(getfeaturevector(test_v))\n",
    "\n",
    "train_token = []\n",
    "test_token = []\n",
    "\n",
    "for token_train in pre_pro_array[:100]:\n",
    "    train_token.append(getfeaturevector(token_train))\n",
    "\n",
    "for token_test in pre_pro_array[100:150]:\n",
    "    test_token.append(getfeaturevector(token_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: (Marks : 5 ) \n",
    "\n",
    "\n",
    "Load the given training data and use the above functions you created to process, to tokenise and to get feature vector.\n",
    "\n",
    "Considering the feature vector as input to the classifier, Train a classifier to classify the sentiment of the tweet correctly.\n",
    "\n",
    "Divide the training data into two sets, to validate your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code here\n",
    "from sklearn import neighbors, datasets\n",
    "from collections import Counter\n",
    "\n",
    "train_token = []\n",
    "test_token = []\n",
    "\n",
    "lables = file_test.Sentiment[:150]\n",
    "train_lables = file_test.Sentiment[:100]\n",
    "test_lables = file_test.Sentiment[100:]\n",
    "\n",
    "\n",
    "for token_train in pre_pro_array[:100]:\n",
    "    train_token.append(getfeaturevector(token_train))\n",
    "\n",
    "for token_test in pre_pro_array[100:150]:\n",
    "    test_token.append(getfeaturevector(token_test))\n",
    "\n",
    "X = train_token\n",
    "# Y = set(pre_pro_array[:100])\n",
    "Y = Counter(pre_pro_array[:100]).most_common(15)\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 20, weights='uniform', algorithm='auto')\n",
    "knn.fit(X, train_lables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    }
   ],
   "source": [
    "p = knn.predict(test_token)\n",
    "#knn.score(p, test_lables)\n",
    "\n",
    "r = []\n",
    "for i in range(len(p)):\n",
    "    r.append(p[i] == t_l[i])\n",
    "    \n",
    "print(r.count(True)/len(r))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_l = []\n",
    "for i in range(len(test_labels)):\n",
    "    t_l.append(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0] == t_l[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: (Marks : 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter crawling using tweepy\n",
    "\n",
    "Use tweepy to get the tweets on real time, which is used as test data for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter account\n",
    "\n",
    "Create a twitter account if you don't have one by going to the link given below:\n",
    "\n",
    "https://twitter.com/i/flow/signup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy: tweepy is the python client for the official Twitter API.\n",
    "Install it using following pip command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets need to be gathered so as to perform Sentiment analysis on those tweets. They can be fetched from Twitter using the Twitter API. \n",
    "\n",
    "In order to fetch tweets through Twitter API, one needs to register an App through their twitter account. Follow these steps for the same:\n",
    "<ul>\n",
    "<li>Open the link given below to create a App through the twitter account.\n",
    "    https://apps.twitter.com\n",
    "<li>click the button: ‚ÄòCreate New App‚Äô\n",
    "<li>Fill the application details. You can leave the callback url field empty.\n",
    "<li>Once the app is created, you will be redirected to the app page.\n",
    "<li>Open the ‚ÄòKeys and Access Tokens‚Äô tab.\n",
    "<li>Copy ‚ÄòConsumer Key‚Äô, ‚ÄòConsumer Secret‚Äô, ‚ÄòAccess token‚Äô and ‚ÄòAccess Token Secret‚Äô.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T14:11:04.892431Z",
     "start_time": "2018-06-29T14:11:04.881562Z"
    }
   },
   "outputs": [],
   "source": [
    "#Replace with your ‚ÄòConsumer Key‚Äô, ‚ÄòConsumer Secret‚Äô, ‚ÄòAccess token‚Äô and ‚ÄòAccess Token Secret‚Äô below. \n",
    "\n",
    "consumer_key = 'w58IaQHhnynfciK3XU9D7s8s2'\n",
    "consumer_secret = 'MJe50jay7VdvBqxIClmjwhdoDPy2QjH9YZWeISxbWLZw5yUNLD'\n",
    "access_token = '212167744-UUDzxFpHbvb30PbolHf7UowqnK6iG9C9CGmxm06x'\n",
    "access_secret = 'VW5b4brUSXS007OTmmPeTeWeK6ulFUZmWGSGZnidb0gNh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to authenticate your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T14:11:07.156485Z",
     "start_time": "2018-06-29T14:11:06.815300Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweepy Cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code gives the search results from twitter for the search string passed to the keyword arguement \"q\" in the tweepy.Cursor. The number passed to the items method of tweepy.Cursor indicates that it gives 100 such tweets, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T14:18:19.093637Z",
     "start_time": "2018-06-29T14:18:07.691456Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt AT_USER during what was a coming-of-age worldcup for both fra and cro, their 1998 meeting is largely remembered for one name:‚Ä¶\n",
      "\n",
      "\n",
      "this sunday we will be showing live coverage of the all-ireland quarter final; limerick v kilkenny, in the lounge b‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åøengland have been fined ¬£50,000 by fifa after 3 players wore 'unauthorised' ankle protector socks. different organ‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER 1. collective motivation üôåüôåüôå 2. character üí™ 3. self-confidence üîùüòÅ we look at what could give cro an edge in sunday's w‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER this weekend, the world cup final will be underway in russia. here's how the tournament ended up there. URL\n",
      "\n",
      "\n",
      "rt AT_USER tonight i wear this old collection jersey. bought it 36 years ago. you know which team i cheer for the 3rd position in the‚Ä¶\n",
      "\n",
      "\n",
      "i liked a AT_USER video URL fut champions weekend league 2 | fifa 18live stream |\n",
      "\n",
      "\n",
      "rt AT_USER st.petersburg grocery ready for fifa bronze today in st.petersburg URL\n",
      "\n",
      "\n",
      "rt AT_USER this weekend, the world cup final will be underway in russia. here's how the tournament ended up there. URL\n",
      "\n",
      "\n",
      "rt AT_USER fifa‚Äôs 2022 world cup in qatar will take place between november 21st and december 18th. the world cup has always been a we‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa president gianni infantino has announced that the 2022 world cup in qatar will take place from november 21st to de‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER which song should be played at the fifa world cup finals 2018? 0% bts ‚Äî fake love 0% exo ‚Äî power 0%‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER st.petersburg grocery ready for fifa bronze today in st.petersburg URL\n",
      "\n",
      "\n",
      "rt AT_USER have you picked your worldcup fan dream team yet? select your xi and you can win some great prizes in the process! find‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER üëâ theelyxion_dot day 2 üëâ burj khalifa led project tomorrow is the last day &amp; fifa world cup. exo exoplanet AT_USER\n",
      "\n",
      "\n",
      "AT_USER AT_USER why would ramsey want to go to lazio... this ain't fifa\n",
      "\n",
      "\n",
      "rt AT_USER having the chance to meet the 2 top soccer ‚öΩ players in the world üåé it's awesome ! tener el chance de saludar a los 2 mejor‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa lovers. let‚Äôs drop some tricks and moves we know so others can know them too. r2 + box - finesse shot (i guess we‚Ä¶\n",
      "\n",
      "\n",
      "thanks AT_USER for lighting up my profile pic. i'm behind croatia all the way, they're going to‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER can you guys believe this is the song that will be played in the closing fifa world cup ceremony? üò≠üò≠ i'm so proud of us! üò≠üíó‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER this weekend, the world cup final will be underway in russia. here's how the tournament ended up there. URL\n",
      "\n",
      "\n",
      "rt AT_USER AT_USER thank you for contacting us. a full list of matches is available on the followin‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER fifa‚Äôs 2022 world cup in qatar will take place between november 21st and december 18th. the world cup has always been a we‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER xherdan shaqiri (26 years old): 5 league titles 5 domestic cups 1 champions league 1 uefa super cup 1 fifa club world c‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER AT_USER thank you for contacting us. a full list of matches is available on the following‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER AT_USER thank you for contacting us. to find out which tv station is showing today's match in y‚Ä¶ URL\n",
      "\n",
      "\n",
      "live streaming fifa world cup 2018 third place play-off belgium vs england bel eng URL\n",
      "\n",
      "\n",
      "AT_USER AT_USER idw to start an argument but if fred wasnt good enough im sure city wouldnt be interes‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER st.petersburg grocery ready for fifa bronze today in st.petersburg URL\n",
      "\n",
      "\n",
      "rt AT_USER despite the jokes and all that, it‚Äôs been a rough year so far. -my little brother scored me 3-0 in fifa -the guys at work‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "AT_USER AT_USER you can't even manage football in kenya premier league leave alone african cup or fifa world cup\n",
      "\n",
      "\n",
      "AT_USER üá≠üá∑ croatia contesalert cro vision game fifa worldcup croatiavsfrance footballfever‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER - more than 70 nigerians stranded in russia, after using fan ids to get into the country with the hope of securing a j‚Ä¶\n",
      "\n",
      "\n",
      "AT_USER AT_USER and what's wrong with that? qatar said they'll provide all the cooling facilities to mak‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER fifa have announced the 2022 world cup will be held from november 21st - december 18th. qatar football stadiums üò≥üî• ht‚Ä¶\n",
      "\n",
      "\n",
      "french fans speak about their 2018 fifa world cup experience: URL via AT_USER\n",
      "\n",
      "\n",
      "rt AT_USER during what was a coming-of-age worldcup for both fra and cro, their 1998 meeting is largely remembered for one name:‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER what the actual fuuuk is going on with this fandom a few days ago we where hella salty about the fifa twitter poll and‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER this weekend, the world cup final will be underway in russia. here's how the tournament ended up there. URL\n",
      "\n",
      "\n",
      "rt AT_USER fifa have announced the 2022 world cup will be held from november 21st - december 18th. qatar football stadiums üò≥üî• ht‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER liverpool will pocket a ¬£2million windfall from the world cup. fifa pays ¬£6,460 per day to clubs for each player released‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa will pay real madrid ‚Ç¨4.3m for their players in the world cup. the second most paid team after manchester city (‚Ç¨‚Ä¶\n",
      "\n",
      "\n",
      "fifa president infantino hails russia's world cup as 'best ever' URL via AT_USER\n",
      "\n",
      "\n",
      "rt AT_USER greed rules again. painful display of a shameless organisation called fifa URL\n",
      "\n",
      "\n",
      "rt AT_USER the fifa world cup is about more than soccer ‚ù§Ô∏è URL\n",
      "\n",
      "\n",
      "fifa world cup 2018: i'm the little man who can, says croatia's luka modric worldcup cro modric URL\n",
      "\n",
      "\n",
      "world cup 2018 - day 25 # date: july 15, 2018 the 2018 fifa world cup finals have arrived! over the past month,‚Ä¶ URL\n",
      "\n",
      "\n",
      "AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER URL\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER the fifa world cup is about more than soccer ‚ù§Ô∏è URL\n",
      "\n",
      "\n",
      "croatia to dedicate 2018 fifa world cup victory to late keeper turina - captain modric URL URL\n",
      "\n",
      "\n",
      "rt AT_USER fifaworldcup2018: when manchester united denied signing AT_USER for just 5 million euros read: URL\n",
      "\n",
      "\n",
      "punjabis are now supporters of croatian team as they feel they all have punjabi names. terevich, merevich, eddevic‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER as i was arguing in my 2005 paper, to make globalization of labor fairer, we need to imitate the fifa rules in many other‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa confirm the 2022 fifa world cup will be played in the winter. URL\n",
      "\n",
      "\n",
      "rt AT_USER fifa have announced the 2022 world cup will be held from november 21st - december 18th. qatar football stadiums üò≥üî• ht‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa knew the world cup can't physically be played during summer in qatar yet they still chose to hold it there and then g‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa announces that the qatar world cup in 2022 will be played from november 21 to december 18 for the first time in history.\n",
      "\n",
      "\n",
      "rt AT_USER üëâ theelyxion_dot day 2 üëâ burj khalifa led project tomorrow is the last day &amp; fifa world cup. exo exoplanet AT_USER\n",
      "\n",
      "\n",
      "rt AT_USER during what was a coming-of-age worldcup for both fra and cro, their 1998 meeting is largely remembered for one name:‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa world cup 2018 is about to end &amp; we all have to wait for 4 years :( URL\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER bts special album track list album: be yourself 1 intro: euphoria 2 fifa world cup poll 3 alexa, rebecca, aria, marty‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER a AT_USER AT_USER a330-300 from brussels to stpetersburg at least today for the bronze medal game at AT_USER\n",
      "\n",
      "\n",
      "rt AT_USER with no reason, i have been grabbed by four security guards and taken out of fifa hotel just minutes ago, in moscow. no rea‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "thanks AT_USER for lighting up my profile pic. i'm behind croatia all the way, they're going to lightuptheworld this fifa worldcup\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you guys believe this is the song that will be played in the closing fifa world cup ceremony? üò≠üò≠ i'm so proud o‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "AT_USER AT_USER fifa, yes. england players donate fees to charity. have done for years.\n",
      "\n",
      "\n",
      "fifa head infantino: 2018 world cup in russia 'best ever': URL via AT_USER\n",
      "\n",
      "\n",
      "rt AT_USER contesalert here‚Äôs another question to test your vision of the game who is gonna win this fifa worldcup? take a g‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER with no reason, i have been grabbed by four security guards and taken out of fifa hotel just minutes ago, in moscow. no rea‚Ä¶\n",
      "\n",
      "\n",
      "AT_USER is that fifa 18?\n",
      "\n",
      "\n",
      "rt AT_USER laurent blanc scoring the golden goal vs paraguay in the 1998 fifa world cup. URL\n",
      "\n",
      "\n",
      "rt AT_USER as it's world cup final week, this one's a biggie - there's a ¬£150 love2shop voucher up for grabs! üõç to enter, rt this tweet,‚Ä¶\n",
      "\n",
      "\n",
      "thanks AT_USER for lighting up my profile pic. i'm behind france all the way, they're going to lightuptheworld this fifa worldcup\n",
      "\n",
      "\n",
      "rt AT_USER official: fifa announce that the 2022 world cup in qatar will take place between november 21st and december 18th. https:‚Ä¶\n",
      "\n",
      "\n",
      "AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER URL\n",
      "\n",
      "\n",
      "rt AT_USER having your dick sucked while playing fifa &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n",
      "\n",
      "\n",
      "rt AT_USER fifa have confirmed that the 2022 worldcup in qatar will take place between november 21st and december 18th. URL\n",
      "\n",
      "\n",
      "nadal vs djok part ii &amp; the fifa chestpain 3rd place playoff today. i feel spoilt by the tv tbqh\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER fifa knew the world cup can't physically be played during summer in qatar yet they still chose to hold it there and then g‚Ä¶\n",
      "\n",
      "\n",
      "rt AT_USER breaking: fifa president gianni infantino has announced that the 2022 world cup in qatar will take place from november 21st‚Ä¶\n",
      "\n",
      "\n",
      "tonight i wear this old collection jersey. bought it 36 years ago. you know which team i cheer for the 3rd positio‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER exo talked about the olympics, dubai, burj khalifa led, fifa voting that we won and their 5th consecutive daesang. so much t‚Ä¶\n",
      "\n",
      "\n",
      "so far it has been an entertaining game full of disappointments . one of the best stars in the world have crashed o‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER bronze is up for grabs in saint petersburg todayü•â who will leave the worldcup after setting foot on the podium: bel or‚Ä¶\n",
      "\n",
      "\n",
      "i liked a AT_USER video URL villa's revenge! - fifa 16 randoms\n",
      "\n",
      "\n",
      "belgium vs england: who'll be the best among the rest? URL beleng fifaworldcup18 URL\n",
      "\n",
      "\n",
      "rt AT_USER fifa referee miku the last miku, thank you for support for the world cup miku :) URL\n",
      "\n",
      "\n",
      "could be bad news for my fifa team selection. bray AT_USER URL\n",
      "\n",
      "\n",
      "rt AT_USER yesterday the members mentioned their 2018 achievements so far like the dubai fountain, 2018 olympics and the fifa world‚Ä¶\n",
      "\n",
      "\n",
      "my team of the tournament of AT_USER : 1. t.courtois (g/k)üáßüá™ 2. b.pavard üá´üá∑ 3. r.varaneüá´üá∑ 4. a.granqvist üá∏üá™‚Ä¶ URL\n",
      "\n",
      "\n",
      "rt AT_USER giroud and mandzukic, strikers who aren't produced by academies but often thrive at international tournaments URL\n",
      "\n",
      "\n",
      "rt AT_USER official: fifa announce that the 2022 world cup in qatar will take place between november 21st and december 18th. URL\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_array = []\n",
    "for i in tweepy.Cursor(api.search, q='fifa', lang = 'en', full_text=True).items(100):\n",
    "    ## print(i._json['text'])\n",
    "    pre_array.append(preprocess_tweets(i._json['text']))\n",
    "    print(preprocess_tweets(i._json['text']), end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also apply the preprocessing steps and obtain the feature vectors for the crawled twitter data.\n",
    "Classify the crawled tweets by passing its feature vector to the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT_USER croatia contesalert cro vision game fifa worldcup croatiavsfrance footballfever URL AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER URL AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER AT_USER URL \n"
     ]
    }
   ],
   "source": [
    "##Your code here\n",
    "\n",
    "\n",
    "tokenizer_array = []\n",
    "for t in pre_array:\n",
    "    tokenizer = word_tokenizer(t)\n",
    "    if tokenizer in stopWords:\n",
    "        tokenizer_array.pop\n",
    "    else: \n",
    "        tokenizer_array.append(tokenizer)\n",
    "        \n",
    "# print(tokenizer_array)\n",
    "test_v = ''\n",
    "for token_tweet in tokenizer_array:\n",
    "        test_v += (' '.join(token_tweet) + ' ')\n",
    "print((test_v))\n",
    "#set_value = set(test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 30)\n"
     ]
    }
   ],
   "source": [
    "print(getfeaturevector(test_v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "train_token = []\n",
    "test_token = []\n",
    "\n",
    "lables = file_test.Sentiment[:150]\n",
    "train_lables = file_test.Sentiment[:100]\n",
    "test_lables = set(file_test.Sentiment[100:])\n",
    "\n",
    "\n",
    "for token_train in pre_array:\n",
    "    train_token.append(getfeaturevector(token_train))\n",
    "\n",
    "for token_test in pre_array[:500]:\n",
    "    test_token.append(getfeaturevector(token_test))\n",
    "\n",
    "X = train_token\n",
    "Y = pre_array[:1000]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X, train_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "p = knn.predict(test_token)\n",
    "#knn.score(p, test_lables)\n",
    "\n",
    "r = []\n",
    "for i in range(len(p)):\n",
    "    r.append(p[i] == t_l[i])\n",
    "    \n",
    "print(r.count(True)/len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_l = []\n",
    "for i in range(len(test_labels)):\n",
    "    t_l.append(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0] == t_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
