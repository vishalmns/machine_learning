{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_2.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"aIRvdphcECv5","colab_type":"text"},"cell_type":"markdown","source":["# Foundations of Artificial Intelligence and Machine Learning\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"2T425AzeECv7","colab_type":"text"},"cell_type":"markdown","source":["#### To be done in the Lab"]},{"metadata":{"id":"8qm9l8jHECv8","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is  PCA feature extraction\n","\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","\n","*   List item\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","*   List item\n","\n","\n","\n","- Extracting eigenvalues and eigenvectors\n","- choosing the best N principal components as features\n"]},{"metadata":{"id":"-nx1ljNLECv9","colab_type":"text"},"cell_type":"markdown","source":["In this experiment we will use the familiar CIFAR-10 dataset."]},{"metadata":{"id":"_GPYn5inECv-","colab_type":"text"},"cell_type":"markdown","source":["They are in a particular python-specific format called pickle. You need not worry about the format's internals, as the site has given the code needed to read such files. The code is given in the first code block below.\n","\n","**The code returns the contents of each data file as a dictionary**."]},{"metadata":{"id":"6NOX9HcPECv_","colab_type":"text"},"cell_type":"markdown","source":["There are 8 files in the cifar-10 directory.\n","\n","    1. batches.meta\n","\n","    2. data_batch_1\n","\n","    3. data_batch_2\t\n","\n","    4. data_batch_3\n","\n","    5. data_batch_4\t\n","\n","    6. data_batch_5\n","\n","    7. readme.html\n","\n","    8. test_batch\n"]},{"metadata":{"id":"PkLixrnGECwA","colab_type":"text"},"cell_type":"markdown","source":["**data** a 50,000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n","\n","**labels** a list of 50,000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data."]},{"metadata":{"id":"HdjIRe18ECwB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Importing required packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sNk1nkLmECwF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Special function to read special files\n","def unpickle(file):\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1HRSecR2ECwJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Visualize the images in CIFAR-10 Dataset\n","## Here get_data unpickles the CIFAR Dataset and stores the data as 50000*3072 dimension in array X \n","## and labels as 50000*1 dimension in array Y. \n","## Visualize function shows the image corresponding to id number.\n","\n","def get_data(file):\n","    my_dict = unpickle(file)\n","    X = my_dict[b'data']\n","    Y = my_dict[b'labels']\n","    names = np.asarray(my_dict[b'filenames'])\n","    list_class = (unpickle(\"../DS/AIML_DS_CIFAR-10_STD/batches.meta\")[b'label_names'])\n","    return X, Y, names,list_class\n","                     \n","\n","def visualize_image(X, Y, names, image_id):\n","    rgb = X[image_id,:]\n","    img = rgb.reshape(3, 32, 32).transpose([1, 2, 0])\n","    plt.imshow(img)\n","    plt.title(names[image_id])\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d8ZAumgkECwM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Read images\n","imgs1, labels1, names1, classes1 = get_data(\"../DS/AIML_DS_CIFAR-10_STD/data_batch_1\")\n","imgs2, labels2, names2, classes2 = get_data(\"../DS/AIML_DS_CIFAR-10_STD/data_batch_2\")\n","imgs3, labels3, names3, classes3 = get_data(\"../DS/AIML_DS_CIFAR-10_STD/data_batch_3\")\n","imgs4, labels4, names4, classes4 = get_data(\"../DS/AIML_DS_CIFAR-10_STD/data_batch_4\")\n","imgs5, labels5, names5, classes5 = get_data(\"../DS/AIML_DS_CIFAR-10_STD/data_batch_5\")\n","imgs_train = np.concatenate((imgs1,imgs2,imgs3,imgs4,imgs5), axis=0)\n","labels_train = np.concatenate((labels1,labels2,labels3,labels4,labels5), axis=0)\n","names_train = np.concatenate((names1,names2,names3,names4,names5), axis=0)\n","classes_train = np.concatenate((classes1,classes2,classes3,classes4,classes5), axis=0)\n","\n","imgs_test, labels_test, names_test, classes_test = get_data(\"../DS/AIML_DS_CIFAR-10_STD/test_batch\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sOh1HItyECwQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Visualize the 10th image\n","pick = 10\n","print(\"Class =\", classes_train[labels_train[pick]])\n","visualize_image(imgs_train, labels_train, names_train, pick)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-i-DA1p6ECwU","colab_type":"text"},"cell_type":"markdown","source":["Lets preprocess the images to get greyscale images, and subtract the mean value from them."]},{"metadata":{"id":"Q4kCgOHSECwU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Function to preprocess the images\n","def preprocess(imgs, mode='train', mean=None):\n","    \n","    #convert image array [50000 x 3072] to image [50000 x 32 x 32 x 3]\n","    imgs = imgs.reshape(imgs.shape[0], 3, 32, 32).transpose([0, 2, 3, 1])\n","    \n","    #convert to grayscale image [50000 x 32 x 32]\n","    imgs = np.dot(imgs[...,:3], [0.299, 0.587, 0.114])\n","    \n","    # If we are dealing with training images, compute the mean\n","    if mode == 'train':\n","        mean = np.mean(imgs, axis=0)\n","    \n","    #subtract by mean image\n","    imgs = imgs - mean\n","    \n","    #convert back to image array [50000 x 1024]\n","    print(imgs.shape)\n","    imgs = imgs.reshape(imgs.shape[0], 1024)\n","    print(imgs.shape)\n","    \n","    return imgs, mean"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-ft-bDOGECwY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Preprocess the images\n","imgs_train, mean = preprocess(imgs_train, mode='train')\n","imgs_test, _ = preprocess(imgs_test, mode='test', mean=mean)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oDcRjhKDECwd","colab_type":"text"},"cell_type":"markdown","source":["### 1. First $N$ Principal Components with maximum eigenvalues\n","\n","As we have learnt in the lecture, PCA finds the set of orthonormal vectors which best\n","describe the distribution of the underlying dataset. In the given dataset, we have $n$\n","images of size $K \\times K$. (We know that $K = 32$, and $n = 50000$ in the image set chosen."]},{"metadata":{"id":"M2risqV8ECwf","colab_type":"text"},"cell_type":"markdown","source":["#### 1.1. Equations related to eigenvalues and eigenvectors\n","\n","The first step towards finding Principal Components is to find the eigenvalues and eigenvectors of the co-variance matrix of our data.\n","\n","From the last subsection, we have the data matrix $\\pmb A$ as an $n \\times K^2$ matrix."]},{"metadata":{"id":"eeP58YiZECwh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Assigning imgs to \"A\"\n","A = imgs_train"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QygvhPpNECwn","colab_type":"text"},"cell_type":"markdown","source":["$\\pmb C$, the covariance matrix of $\\pmb A$, is shown in the below equation:\n","\n","$$\n","% C = \\frac{1}{n}\\sum_{i=1}^{n}A^T.A\n","\\pmb C = \\frac{1}{n}\\ (\\pmb A^T.\\pmb A)\n","$$\n","\n","This can be coded in Python as : `C = 1 / A.shape[0] * np.dot(A.T, A)`. Its size is $K^2 \\times K^2$, i.e. the shape of $\\pmb C$ is $(K^2, K^2)$.\n","\n","The eigenvaues and eigenvectors of $\\pmb C$ can be found in Python as: `w, v = np.linalg.eig(C)`, where `w` are the eigenvalues, and `v` is the matrix of eigenvectors. They can also be computed using Singular Value Decomposition: `np.linalg.svd(A,full_matrices=False, compute_uv=True)` \n","\n","Since $\\pmb C$ is a $K^2 \\times K^2$ matrix, there should be $K^2$ eigenvalues, and $K^2$ eigenvectors each of $K^2$ dimensions. So `w` is a numpy array of shape ($K^2$,), i.e. it contains $K^2$ number of eigenvalues. `v` is a numpy array of shape ($K^2$, $K^2$), where each **column** of `v` is an eigenvector. So there are $K^2$ eigenvectors, each of shape ($K^2$,)."]},{"metadata":{"id":"9dgi0mLmECwo","colab_type":"text"},"cell_type":"markdown","source":["### 1.2. Problem\n","\n","But, since our images are of size $32\\times32$, i.e. $K = 32$, we shall be finding eigenvalues and eigenvectors of a $1024\\times1024$-sized covariance matrix $C$. Computing $C$ and then $w$ (the eigenvalues) and $V$ (the eigenvectors) is an intractable task and may result in a Memory Error depending on the dataset used. (To be discussed in the next lecture)"]},{"metadata":{"id":"GgA7cN75ECwp","colab_type":"text"},"cell_type":"markdown","source":["### 1.3. Find eigenvalues and eigenvectors"]},{"metadata":{"id":"EYkZtURNECwq","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 1 : Write a function to find the eigenvalues and eigenvectors of the covariance of matrix A without finding C.**\n","\n","The input to this function is a data matrix $\\pmb A$ (i.e. `A = imgs_train`), and the outputs are `w` (eigenvalues) and `V` (eigenvectors) of $\\pmb A^T.\\pmb A$."]},{"metadata":{"id":"eeqc7IUXECwr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def find_eigenvalues_and_eigenvectors(A):\n","        #################################################\n","    ##     C = A.T.dot(A) / (len(A.T)-1) \n","    ##     e, u = np.linalg.eig(C)\\n,\n","    ##     w = e\\n,\n","    ##     v = np.dot(A.T, u)\\n,\n","    ##     return w, v\n","    #################################################\n","    ## The above approach using np.linalg.eig() might result in errors depending on the nature of C,\n","    ## hence we find solution using SVD (singular value decomposition)\n","    ## SVD decomposes A[1024 x 1024] into UwV,\n","    ## where U is [1024 x 1024], w is [1024,] and V is [1024 x 1024]\n","    ## the columns of U are the eigenvectors\n","    #################################################\n","    \n","    # Your code here\n","    ??????????????\n","    return w, V"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fnSkAsD4ECww","colab_type":"text"},"cell_type":"markdown","source":["Using the above function, let us find the eigenvalues and eigenvectors in the training data."]},{"metadata":{"id":"d1X0e5d4ECww","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["eigenvalues_train, eigenvectors_train = find_eigenvalues_and_eigenvectors_simply(imgs_train)\n","print(eigenvalues_train.shape)\n","print(eigenvectors_train.shape)\n","eigenvalues_test, eigenvectors_test = find_eigenvalues_and_eigenvectors_simply(imgs_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iqwWZlS3ECw0","colab_type":"text"},"cell_type":"markdown","source":["### 1.4. Reordering, normalizing\n","\n","But, since we found the eigenvalues and eigenvectors in a roundabout way, we need to:\n","- reorder them so that they are in descending order of eigenvalues,\n","- normalize the eigenvectors so that their norms are 1."]},{"metadata":{"id":"94XWPfSiECw3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# REORDER\n","\n","# Find the required order of indices to make decreasing order of eigenvalue\n","sort_index = np.argsort(eigenvalues_train)[::-1]\n","\n","# Use the calculated order of indices to reorder eigenvalues and eigenvectors\n","eigenvalues_train = eigenvalues_train[sort_index]\n","eigenvectors_train = eigenvectors_train[:, sort_index]\n","\n","# Find the required order of indices to make decreasing order of eigenvalue\n","sort_index = np.argsort(eigenvalues_test)[::-1]\n","\n","# Use the calculated order of indices to reorder eigenvalues and eigenvectors\n","eigenvalues_test = eigenvalues_test[sort_index]\n","eigenvectors_est = eigenvectors_test[:, sort_index]\n","\n","\n","#see top 3 eigenvalues\n","print(eigenvalues_train[:3])\n","print(eigenvectors_train[:3])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l5JrEmzUECw8","colab_type":"text"},"cell_type":"markdown","source":["As can be seen, the eigenvalues are in decreasing order. Let us check the norm of an eigenvector, it should be 1:"]},{"metadata":{"id":"KHEjGioiECw-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(np.linalg.norm(eigenvectors_train[:,1]))    #checking if norm is 1\n","# NORMALIZE (no need to normalize when using SVD as it returns normalized eigenvectors)\n","# eigenvectors_train = eigenvectors_train / np.linalg.norm(eigenvectors_train, axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aVC2xqFFECxB","colab_type":"text"},"cell_type":"markdown","source":["### 1.5. Computing good value for $N$"]},{"metadata":{"id":"5zZnCxoMECxC","colab_type":"text"},"cell_type":"markdown","source":["In the given dataset, there are as many eigenvectors as the number of training examples. This can be verified by:"]},{"metadata":{"id":"a_J_GARyECxC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["eigenvectors_train.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eMv-fFeHECxG","colab_type":"text"},"cell_type":"markdown","source":["Since each column is an eigenvector, there are 1024 eigenvectors, each of 1024 dimensions. But usually, a smaller number $N$ of eigenvectors is chosen as a basis to make feature vectors.\n","\n","To decide the on the number $N$, i.e. the number of most important eigenvectors to keep as the basis, the cumulative sum of eigenvalues (assuming they are in decreasing order) divided by the total sum of eigenvalues, vs. the number of eigenvalues considered ($N$) is plotted.\n","\n","This plot shall show the fraction of total variance retained ($r$) vs. the number of eigenvalues considered ($N$). This way, the plot gives a good understanding of the point of diminishing returns, i.e. the point where little variance is retained by retaining additional eigenvalues.\n","\n","This can be understood by the following equation:\n","\n","$$r = \\frac{\\sum_{k=1}^{N}\\lambda_k}{\\sum_{k=1}^{n}\\lambda_k},\\ \\ \\ \\  N << n$$\n","\n","Plotting $r$ vs $N$ shall give a good idea of the impact of varying $N$ on $r$.\n","\n","Let's say we want to retain only 80% of the variance involved. Then we should look for the minimum value of $N$ for which $r > 0.8$."]},{"metadata":{"id":"JKIBlza0ECxG","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 2 : Edit 1 line of code to calculate r, plot $r$ vs $M$.**"]},{"metadata":{"id":"UQ82rS8NECxI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Plot r vs M\n","# Values of M to consider: 1, 2,..., n\n","M = np.array(range(1, imgs_train.shape[1] + 1))\n","\n","# Calculate r for all values of M\n","# Your code here\n","# Hint: Look for \"numpy cumulative sum\"\n","r = ???????????\n","\n","# Plot r vs M\n","plt.plot(M, r)\n","# We take only first 1024 eigenvectors because\n","# rest all correspond to eigen value 0\n","plt.xlabel(\"M\", fontsize=20)\n","plt.ylabel(\"r\", fontsize=20)\n","plt.grid(\"on\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4JA835ysECxN","colab_type":"text"},"cell_type":"markdown","source":["We can see from the plot that an $M$ value of around 25 (out of 1024) gives an $r$ value of 0.8."]},{"metadata":{"id":"4nC1RILTECxN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["r[25]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P7vbLTshECxX","colab_type":"text"},"cell_type":"markdown","source":["---- **(If it does not, please recheck your code.)**\n","\n","So let us choose $N = 25$."]},{"metadata":{"id":"Uuiv_2kDECxY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["N = 25"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dJlOT60sECxb","colab_type":"text"},"cell_type":"markdown","source":["This means we are choosing only $N$ **principal components**. In other words, we are choosing those $N$ types of information that are most important (preserving $80\\%$ of the class variance but using only top 25 eigenvectors out of 1024).\n","\n","Let us note the first N principal components, i.e. the first N eigenvectors:"]},{"metadata":{"id":"Vq6L5f8fECxc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["pca_vectors_train = eigenvectors_train[:, :N]\n","pca_vectors_test = eigenvectors_test[:, :N]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qKXs6Z__ECxf","colab_type":"text"},"cell_type":"markdown","source":["### 1.6. Finding features using first $N$ Principal Components\n","\n","Since we are using the most important eigenvectors as the _basis_ vectors, we need to project the data into these basis components to find the relevant features. We do this by finding the dot product of the data maxtrix and the matrix of the most important eigenvectors.\n","\n","We know that the data is of shape $n \\times K^2$. We also know that the `pca_vectors` matrix is of shape $K^2 \\times N$."]},{"metadata":{"id":"skgh3-f9ECxf","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 3: Edit 1 line of code to find the pca_features of the data:**"]},{"metadata":{"id":"x1ciG-1NECxg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["pca_features_train = ?????????????\n","pca_features_test = ???????????\n","\n","print(imgs_train.shape)\n","print(pca_features_train.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AJ5lgpBmECxj","colab_type":"text"},"cell_type":"markdown","source":["Therefore, we have effectively reduced the dimension of features from 1024 to 25 while preserving 80% of variance in the data. We can see that we have transformed our data from $n \\times K^2$ [50000 x 1024] to $n \\times N$ [50000 x 25] where $N$ is the chosen number of principal components.\n","\n","### 1.7. Visualizing Variance captured by topmost eigenvectors##"]},{"metadata":{"id":"-cqWco3CECxk","colab_type":"text"},"cell_type":"markdown","source":["Let us visualize the range of values that each eigenvector is able to capture. This shall give us a good idea of the amount of variance each eigenvector is capturing."]},{"metadata":{"id":"uqEq-OfhECxl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["number_of_samples =  100\n","plt.figure(figsize=(10, 5))\n","plt.subplot(141)\n","plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 0], c='r')\n","plt.xlabel('PCA 1st dimension')\n","plt.ylim([-2500, 2500])\n","plt.subplot(142)\n","plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 1], c='y')\n","plt.xlabel('PCA 2nd dimension')\n","plt.ylim([-2500, 2500])\n","plt.subplot(143)\n","plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 3], c='y')\n","plt.xlabel('PCA 3rd dimension')\n","plt.ylim([-2500, 2500])\n","plt.subplot(144)\n","plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 4], c='y')\n","plt.xlabel('PCA 4th dimension')\n","plt.ylim([-2500, 2500])\n","plt.title('Visualizing variances captured by eigenvectors')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WHboUEvuECxo","colab_type":"text"},"cell_type":"markdown","source":["As can be seen, the amount of variance captured is decreasing along each eigenvector. Something like this:\n","\n","<img src=\"var.gif\">"]},{"metadata":{"id":"hymWadWHECxp","colab_type":"text"},"cell_type":"markdown","source":["### 2. k Nearest Neighbours\n","\n","By now, we are quite familiar with the kNN algorithm, so we shall use this to classify test images.\n","\n","**Problem:** Given a test image, classify its label?\n","\n","**Solution:** We shall give the test image's features to a kNN model, and take a majority vote on the k nearest features in the training set.\n","\n","Here is the familiar kNN code (modified slightly to feed train_features and train_labels independently) and also the code for multiclass_classifier done in previous session:"]},{"metadata":{"id":"UpOPHmB0ECxp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import math\n","import collections\n","\n","def dist(a, b):\n","    sqSum = 0\n","    for i in range(len(a)):\n","        sqSum += (a[i] - b[i]) ** 2\n","    return math.sqrt(sqSum)\n","\n","def kNN(k, train_features, train_labels, given_feature):\n","    distances = []\n","    for t in range(len(train_features)):\n","        distances.append((dist(train_features[t], given_feature), train_labels[t]))\n","    distances.sort()\n","    return distances[:k]\n","\n","def kNN_classify(k, train_features, train_labels, given_feature):\n","    tally = collections.Counter()\n","    for nn in kNN(k, train_features, train_labels, given_feature):\n","        tally.update(str(int(nn[-1])))\n","    return int(tally.most_common(1)[0][0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZZE__CgmECxt","colab_type":"text"},"cell_type":"markdown","source":["For example, let's use $k = 3$, and predict the class of the $1^{st}$ image in the test set:"]},{"metadata":{"id":"VUV-smBxECxu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["kNN_classify(3,  pca_features_train, labels_train, pca_features_test[1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L8pjzVciECx1","colab_type":"text"},"cell_type":"markdown","source":["We know that the label of the image is:"]},{"metadata":{"id":"U_Cdh7LHECx3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["labels_test[1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IrcUsYp2ECx8","colab_type":"text"},"cell_type":"markdown","source":["Thus, kNN has classified this image correctly."]},{"metadata":{"id":"5YQCRlkoECx9","colab_type":"text"},"cell_type":"markdown","source":["##### Use kNN to classify the first 10 test images using pca_features"]},{"metadata":{"id":"xoAlW2SiECx9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["predicted_labels_test = []\n","\n","# Predict labels on the test set\n","for i in range(10):\n","    print(\"Predicting\", i+1, \"of 10\")\n","    predicted_labels_test.append(kNN_classify(3, pca_features_train, labels_train, pca_features_test[i]))\n","\n","# Find accuracy\n","kNN_test_accuracy_pca = np.mean(np.array(predicted_labels_test) == np.array(labels_test[:10]))\n","print(\"Accuracy =\", kNN_test_accuracy_pca)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dksFrHkPECyA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["predicted_labels_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KmIFAhVjECyC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["labels_test[:10]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HB0IqWMLECyF","colab_type":"text"},"cell_type":"markdown","source":["Thus, using top 25 pca features, we got 0.3 accuracy on the first 10 test images."]},{"metadata":{"id":"CKdCKpUtECyF","colab_type":"text"},"cell_type":"markdown","source":["## Exercise 4: Experiment with different values of $N$ (5, 10, 20, 40, 100), and observe the effects on accuracy ###"]},{"metadata":{"id":"kWpblCGjECyG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hZHRXbK9ECyI","colab_type":"text"},"cell_type":"markdown","source":["### Summary"]},{"metadata":{"id":"syNoCMTWECyJ","colab_type":"text"},"cell_type":"markdown","source":["Thus PCA is used to identify a hyperplane that lies closest to the data points, and project the data onto it."]}]}