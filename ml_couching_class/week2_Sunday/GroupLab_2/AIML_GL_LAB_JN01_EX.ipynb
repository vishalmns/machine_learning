{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Artificial Intelligence and Machine Learning\n",
    "## A Program by IIIT-H and TalentSprint\n",
    "#### To be done in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this lab is to learn how to perform Data preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T06:42:31.529286Z",
     "start_time": "2018-06-02T06:42:31.524108Z"
    }
   },
   "source": [
    "We will be using district wise demographics, enrollments, school and teacher indicator data to predict whether the literacy rate is high / medium / low in each district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is an important step of solving every machine learning problem. Most of\n",
    "the datasets used with Machine Learning problems need to be processed / cleaned / transformed\n",
    "so that a Machine Learning algorithm can be trained on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different steps involved for Data Preprocessing. These steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Data Cleaning → In this step the primary focus is on\n",
    "        \n",
    "        -Handling missing data\n",
    "        -Handling noisy data\n",
    "        -Detection and removal of outliers\n",
    "    \n",
    "    2. Data Integration → This process is used when data is gathered from various data sources\n",
    "    and data are combined to form consistent data. This data after performing cleaning is used\n",
    "    for analysis.\n",
    "    \n",
    "    3. Data Transformation → In this step we will convert the raw data to a required scale\n",
    "    according to the need of the model we are building. There are many options used for\n",
    "    transforming the data:\n",
    "       \n",
    "        -Normalization\n",
    "        -Aggregation\n",
    "        -Generalization\n",
    "        \n",
    "    4. Data Reduction → After data transformation, redundancy in the data as \n",
    "    to be removed by feature selection based on correlation/ PCA etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T11:01:38.734682Z",
     "start_time": "2018-06-22T11:01:38.728849Z"
    }
   },
   "source": [
    "In the end of this lab, you need to predict the Overall district wise Literacy rate category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Marks  = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - (2 Marks)\n",
    "We have four different files\n",
    "\n",
    "* Districtwise_Basicdata.csv\n",
    "* Districtwise_Enrollment_details_indicator.csv\n",
    "* Districtwise_SchoolData.csv\n",
    "* Districtwise_Teacher_indicator.csv\n",
    "\n",
    "These files contain the neccesary data to solve the problem.\n",
    "Load all the files correctly, after observing the header level details, data records etc\n",
    "\n",
    "Hint : Use read_csv from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2  - (3 Marks)\n",
    "\n",
    " * Remove the unwanted columns, which are unlikely to contribute for the prediction of overall literacy category(Manual Feature Selection)\n",
    "\n",
    " * The data is present in different files. We need to integrate all of the data into a single data frame. \n",
    " * For example all the data of the Anantapur district (AP) should form a single row.     \n",
    " * Recall your SQL: create or identify a unique identifier for each row in all the data frames and use it to combine the data.\n",
    "\n",
    "*One possibility -- by no means the best-- is (year, state code, district code) combination* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 - (2 Marks)\n",
    "\n",
    "Carry out the following steps to clean the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Overall_lit is our label. Without it we cannot meaningfully train. \n",
    "    + So we can delete the rows with missing values in that column.\n",
    "    + **Bonus** What else can we do with it?\n",
    "\n",
    "* Consider replacing the missing values in any other column \n",
    "    + possibly with mean/median/mode.\n",
    "\n",
    "* Convert categorical values to numerical values.\n",
    "\n",
    "    + For example: If a feature contains categorical values such as dog, cat, mouse etc then replace them with 1, 2, 3 and so on or use one hot encoding (your judgement)\n",
    "\n",
    "*Hint*  Read up `pandas.fillna()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 - (3 Marks)\n",
    "\n",
    "Use the functions below to adjust the outliers\n",
    "\n",
    "**smooth_out**\n",
    "\n",
    " * takes a  dataframe as input,\n",
    " * calculates the mean ($\\mu$) and standard deviation ($\\sigma$) of every column\n",
    " * and replaces outliers with boundary values.\n",
    "\n",
    "Outliers are those beyond $2\\sigma$ from $\\mu$\n",
    "\n",
    "<img src=\"normal_dist.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clip and clamp the data\n",
    "def clip_clamp(x, mean, sd):\n",
    "    if x < mean - 2 * sd :\n",
    "        return mean - 2 * sd\n",
    "    elif x > mean + 2 * sd :\n",
    "        return mean + 2 * sd\n",
    "    else :\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to smooth the data\n",
    "def smooth_out(Total_data):\n",
    "    for i in Total_data.columns:\n",
    "        mean = np.mean(Total_data[i].values, axis=0)\n",
    "        sd = np.std(Total_data[i].values, axis=0)\n",
    "        corrected = np.array([clip_clamp(x, mean, sd) for x in Total_data[i].values])\n",
    "        Total_data[i] = pd.Series(corrected, index=Total_data[i].index)\n",
    "    return Total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 - (2 Marks)\n",
    "\n",
    "Use the function below (corr_features) to identify uncorrelated features and remove the remaining (dependent) features\n",
    "\n",
    "* corr_features takes pandas dataframe, columns in the dataframe and bar (correlation co-efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T15:18:25.800312Z",
     "start_time": "2018-06-21T15:18:25.759272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to find uncorrelated features\n",
    "def corr_features(df, cols, bar=0.9):\n",
    "    correleated = set()\n",
    "    for pos, aCol in enumerate(cols[:-1]):\n",
    "        if aCol in correlated: ## aCol has already been found dependent\n",
    "            continue\n",
    "        else: \n",
    "            for bCol in cols[pos + 1:]:\n",
    "                score = df[aCol].corr(df[bCol])\n",
    "                if -bar < score < bar:\n",
    "                    pass ## The correlation coefficient is within set limits\n",
    "                else:\n",
    "                    correlated.add(bCol)\n",
    "    return list(set(cols) - correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6 - (3 Marks)\n",
    "\n",
    "Perform Mean Correction and Standard Scaling feature/column wise on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7 - (2 Marks)\n",
    "\n",
    "Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8 - (3 Marks)\n",
    "\n",
    "Apply k-NN and Decision Tree classifiers on the preprocessed data and figure out which classifier gives the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace any of the above given functions and get correct results to get excellence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
