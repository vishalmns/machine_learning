{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiment_1.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"K9mGrC7FircR","colab_type":"text"},"cell_type":"markdown","source":["# Foundations of Artificial Intelligence and Machine Learning\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"DVulyFmxircT","colab_type":"text"},"cell_type":"markdown","source":["\n","#### To be done in the Lab"]},{"metadata":{"id":"MwrTaSqYircU","colab_type":"text"},"cell_type":"markdown","source":["The objective of this experiment is to learn how to tune hyper parameters."]},{"metadata":{"id":"hqNL7H6pircV","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://)In this experiment, we will use Wisconsin Breast Cancer data to detect malignant cells.\n","\n","### Data Source\n","\n","https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n","\n","The data has been modified:\n","  * The id field has been removed\n","  * The diagnosis field has been moved to the end"]},{"metadata":{"id":"htURP_5NircX","colab_type":"text"},"cell_type":"markdown","source":["### Data Attributes\n","\n","Number of instances: 569 \n","\n","Number of attributes: 31 (diagnosis, 30 real-valued input features)\n","\n","Ten real-valued features are computed for each cell nucleus:\n","\n","\ta) radius (mean of distances from center to points on the perimeter)\n","\tb) texture (standard deviation of gray-scale values)\n","\tc) perimeter\n","\td) area\n","\te) smoothness (local variation in radius lengths)\n","\tf) compactness (perimeter^2 / area - 1.0)\n","\tg) concavity (severity of concave portions of the contour)\n","\th) concave points (number of concave portions of the contour)\n","\ti) symmetry \n","\tj) fractal dimension (\"coastline approximation\" - 1)\n","\n","The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.  For instance, field 1 is Mean Radius, field 11 is Radius SE, field 21 is Worst Radius. All feature values are recoded with four significant digits.\n","\n","The last field is diagnosis: M for Malignant and B for Benign\n","\n","Class distribution: 357 benign, 212 malignant"]},{"metadata":{"id":"2zdlcLt9ircY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["CANCERDATA = \"../Datasets/AIML_DS_WDBC_NOIDFIELD.data\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"RLTCVZxlircc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","pd.read_csv(CANCERDATA, header=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qcIoHy4fircf","colab_type":"text"},"cell_type":"markdown","source":["Obviously we cannot do much plotting with these many dimensions :-) Let us recollect the code we have already used."]},{"metadata":{"id":"L34jjlqPirch","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import math\n","import collections\n","def dist(a, b):\n","    sqSum = 0\n","    for i in range(len(a)):\n","        sqSum += (a[i] - b[i]) ** 2\n","    return math.sqrt(sqSum)\n","# ------------------------------------------------ #\n","# We are assuming that the label is the last field #\n","# If not, munge the data to make it so!            #\n","# ------------------------------------------------ #\n","def kNN(k, train, given):\n","    distances = []\n","    for t in train:\n","        distances.append((dist(t[:-1], given), t[-1]))\n","    distances.sort()\n","    return distances[:k]\n","\n","def kNN_classify(k, train, given):\n","    tally = collections.Counter()\n","    for nn in kNN(k, train, given):\n","        tally.update(nn[-1])\n","        print(\"Tally is \", nn[-1])\n","    return tally.most_common(1)[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cVa75e3uirck","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["wdbc = pd.read_csv(CANCERDATA, header=None)\n","wdbc.values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LbTga5TOircn","colab_type":"text"},"cell_type":"markdown","source":["But where are the test data? In practice, as a designer of the algorithm you are given only one set of data. The real test data is with your teacher/examiner/customer. The standard way is to create a small validation data from your training data and use it for evaluating the performance and also for parameter tuning. Let us *randomly* split the data into 80:20 ratio. We will use 80% for training and the rest 20% for evaluating the performance. "]},{"metadata":{"id":"UVD4XkoRirco","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import random\n","TRAIN_TEST_RATIO = 0.8\n","train = []\n","test = []\n","data = wdbc.values\n","for one in data:\n","    if random.random() < TRAIN_TEST_RATIO:\n","        train.append(one)  \n","    else:\n","        test.append(one)\n","## We generate a random number for for each data item.\n","## if it is < 0.8 it is taken as a training data else as testing"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HYhey7EWirct","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(kNN_classify(5, train, test[0])[0], test[0][-1])\n","print(kNN_classify(5, train, test[4])[0], test[4][-1])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8z7-Nqdpircz","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 1** :: Using all the attributes/features, predict the patients in the testdata as malignant or benign. Use K = 5 and Euclidean distance. Find the accuracy (as percentage) on the test data."]},{"metadata":{"id":"pVwLycCBircz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZEwhskj5irc4","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 2** :: (*Cross Validation*) Repeat the above (creating random partitions and evaluating the performance) 5 times. You will see that they vary in different trials. An average of these attempts is in fact a better estimate."]},{"metadata":{"id":"mi07DXiKirc5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CKhjWcnCirc-","colab_type":"text"},"cell_type":"markdown","source":["**Exercise 3** :: Vary K from 3 to 11 and find the best K. In practice, we will use similar ideas for finding best hyper-parameters. We will see more at a later stage."]},{"metadata":{"id":"QNNyMa6virc_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eB_POF5WirdC","colab_type":"text"},"cell_type":"markdown","source":["## Summary\n","We have now seen how KNN works in practice on a real data. We also have some hints on how to find the “best” hyper-parameters by “internally testing” the performance on a small portion of the data that we have. Also we have seen that there could be ”statistical variation” in performance across different splits of data and a more reliable way to measure the performance is to find the average performance (classification accuracy in this case) across multiple splits of data."]}]}